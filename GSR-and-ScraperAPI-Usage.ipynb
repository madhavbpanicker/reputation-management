{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f882c9e4-0a91-4f3c-a70e-800b4b3e4a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more items in response.\n",
      "Results saved to /home/madhavbpanicker/Documents/Scrape_project/Google-Search-Results/reputation-management-gsr-toyota-\"forum\"-reviews-.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Constants\n",
    "API_KEY = \"AIzaSyAubcx_4--W1ynVOBNCmhZyU_yOO6xPwFs\"  # Replace with your Google API key\n",
    "CSE_ID = \"2454833207fbd4f37\"  # Replace with your Custom Search Engine ID\n",
    "SEARCH_QUERY = \"toyota \\\"forum\\\" reviews \"  # Replace with your search query\n",
    "NUM_RESULTS = 100  # Total number of results needed (max 100)\n",
    "SEARCH_FILE_FOLDER_PATH=\"/home/madhavbpanicker/Documents/Scrape_project/Google-Search-Results/\"\n",
    "PROJECT_NAME=\"reputation-management\"\n",
    "def fetch_google_results(api_key, cse_id, query, num_results, date_restrict=None):\n",
    "    \"\"\"Fetches results from Google Custom Search JSON API.\"\"\"\n",
    "    results = []\n",
    "    start_index = 1\n",
    "    while len(results) < num_results:\n",
    "        # Ensure not to request more than the remaining results needed\n",
    "        max_results = min(num_results - len(results), 10)\n",
    "\n",
    "        url = (\n",
    "            f\"https://www.googleapis.com/customsearch/v1\"\n",
    "            f\"?key={api_key}&cx={cse_id}&q={query}&start={start_index}\"\n",
    "        )\n",
    "\n",
    "        # Add date restriction if provided\n",
    "        if date_restrict:\n",
    "            url += f\"&dateRestrict={date_restrict}\"\n",
    "\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        if \"items\" not in data:\n",
    "            print(\"No more items in response.\")\n",
    "            break\n",
    "\n",
    "        # Append results\n",
    "        for item in data[\"items\"]:\n",
    "            # Extract the publication date from the pagemap if available\n",
    "            date = None\n",
    "            if \"pagemap\" in item and \"metatags\" in item[\"pagemap\"]:\n",
    "                metatags = item[\"pagemap\"][\"metatags\"]\n",
    "                for tag in metatags:\n",
    "                    date = tag.get(\"article:published_time\") or tag.get(\"pubdate\")\n",
    "                    if date:\n",
    "                        break\n",
    "\n",
    "            results.append({\n",
    "                \"Title\": item.get(\"title\"),\n",
    "                \"Link\": item.get(\"link\"),\n",
    "                \"Description\": item.get(\"snippet\"),\n",
    "                \"Date\": date or \"N/A\",  # Default to \"N/A\" if no date is found\n",
    "            })\n",
    "\n",
    "        # Update start index for the next batch (increment by 10)\n",
    "        start_index += max_results\n",
    "\n",
    "        # Google Custom Search JSON API allows a maximum of 100 results\n",
    "        if start_index > 100:\n",
    "            print(\"Reached maximum retrievable results (100).\")\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_results_to_json(results, file_name, filename):\n",
    "    \"\"\"Saves results to a JSON file in the specified folder.\"\"\"\n",
    "    # Ensure the folder exists\n",
    "    os.makedirs(file_name, exist_ok=True)\n",
    "\n",
    "    # Create full file path\n",
    "    file_path = os.path.join(file_name, filename)\n",
    "\n",
    "    # Save results to JSON\n",
    "    with open(file_path, mode='w', encoding='utf-8') as file:\n",
    "        json.dump(results, file, indent=4, ensure_ascii=False)\n",
    "    print(f\"Results saved to {file_path}\")\n",
    "\n",
    "# Fetch results\n",
    "DATE_RESTRICT = 'd2'\n",
    "search_results = fetch_google_results(API_KEY, CSE_ID, SEARCH_QUERY, NUM_RESULTS, DATE_RESTRICT)\n",
    "\n",
    "# Replace spaces with dashes for the folder and file name\n",
    "file_name = SEARCH_QUERY.replace(' ', '-')\n",
    "output_filename = f\"{PROJECT_NAME}-gsr-{file_name}.json\"\n",
    "\n",
    "# Save to JSON in the folder\n",
    "if search_results:\n",
    "    save_results_to_json(search_results, SEARCH_FILE_FOLDER_PATH, output_filename)\n",
    "else:\n",
    "    print(\"No results to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7c3ca-d340-4280-abfa-0b984b599b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# ScraperAPI endpoint and API key\n",
    "SCRAPER_API_URL = \"http://api.scraperapi.com\"\n",
    "API_KEY = \"190d8602cbff5cbcfc555cbae04aeddc\"\n",
    "\n",
    "#Folder Path and Project Name Definition\n",
    "RAW_DATA_FOLDER_PATH=\"/home/madhavbpanicker/Documents/Scrape_project/Website-Data-Raw/\"\n",
    "PROJECT_NAME=\"reputation-management\"\n",
    "SEARCH_FILE_FOLDER_PATH=\"/home/madhavbpanicker/Documents/Scrape_project/Google-Search-Results/\"\n",
    "\n",
    "# Function to fetch page content using ScraperAPI\n",
    "def fetch_content(url):\n",
    "    try:\n",
    "        params = {\n",
    "            \"api_key\": API_KEY,\n",
    "            \"url\": url\n",
    "        }\n",
    "        response = requests.get(SCRAPER_API_URL, params=params, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            print(f\"Failed to fetch {url}: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Read the CSV file with URLs\n",
    "#input_file = f\"gsr-{SEARCH_QUERY.replace(' ','-')}.csv\"\n",
    "input_file = f\"{SEARCH_FILE_FOLDER_PATH}{output_filename}\"\n",
    "df = pd.read_json(input_file, orient='records')\n",
    "\n",
    "# Add columns for page content and fetch date\n",
    "df['page_content'] = \"\"\n",
    "df['fetch_date'] = \"\"\n",
    "\n",
    "# Loop through each URL and fetch content\n",
    "for index, row in df.iterrows():\n",
    "    url = row['Link']\n",
    "    print(f\"Fetching content for: {url}\")\n",
    "    \n",
    "    # Fetch content and add current date\n",
    "    content = fetch_content(url)\n",
    "    fetch_date = datetime.now().strftime('%Y-%m-%d')  # Get current date in YYYY-MM-DD format\n",
    "    \n",
    "    # Update DataFrame\n",
    "    df.at[index, 'page_content'] = content\n",
    "    df.at[index, 'fetch_date'] = fetch_date\n",
    "    \n",
    "    time.sleep(2)  \n",
    "\n",
    "# Save the updated DataFrame with page content to a new CSV file\n",
    "output_file = f\"{RAW_DATA_FOLDER_PATH}{PROJECT_NAME}-scraped_results-{SEARCH_QUERY.replace(' ','-')}.json\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(df, f, ensure_ascii=False, indent=4)  # Escapes special characters and adds indentation\n",
    "\n",
    "print(f\"Scraping completed. Results saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
