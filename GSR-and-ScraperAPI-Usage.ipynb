{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f882c9e4-0a91-4f3c-a70e-800b4b3e4a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached maximum retrievable results (100).\n",
      "Results saved to /home/madhavbpanicker/Documents/Scrape_project/Google-Search-Results/reputation-management-gsr-hyundai-\"forum\"-reviews.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Constants\n",
    "API_KEY = \"AIzaSyAubcx_4--W1ynVOBNCmhZyU_yOO6xPwFs\"  # Replace with your Google API key\n",
    "CSE_ID = \"2454833207fbd4f37\"  # Replace with your Custom Search Engine ID\n",
    "SEARCH_QUERY = \"hyundai \\\"forum\\\" reviews\"  # Replace with your search query\n",
    "NUM_RESULTS = 100  # Total number of results needed (max 100)\n",
    "SEARCH_FILE_FOLDER_PATH = \"/home/madhavbpanicker/Documents/Scrape_project/Google-Search-Results/\"\n",
    "PROJECT_NAME = \"reputation-management\"\n",
    "\n",
    "\n",
    "def fetch_google_results(api_key, cse_id, query, num_results, date_restrict=None):\n",
    "    \"\"\"Fetches results from Google Custom Search JSON API.\"\"\"\n",
    "    results = []\n",
    "    start_index = 1\n",
    "    while len(results) < num_results:\n",
    "        max_results = min(num_results - len(results), 10)\n",
    "\n",
    "        url = (\n",
    "            f\"https://www.googleapis.com/customsearch/v1\"\n",
    "            f\"?key={api_key}&cx={cse_id}&q={query}&start={start_index}\"\n",
    "        )\n",
    "\n",
    "        # Add date restriction if provided\n",
    "        if date_restrict:\n",
    "            url += f\"&dateRestrict={date_restrict}\"\n",
    "\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        if \"items\" not in data:\n",
    "            print(\"No more items in response.\")\n",
    "            break\n",
    "\n",
    "        # Append results\n",
    "        for item in data[\"items\"]:\n",
    "            # Extract the publication date from the pagemap if available\n",
    "            date = None\n",
    "            if \"pagemap\" in item and \"metatags\" in item[\"pagemap\"]:\n",
    "                metatags = item[\"pagemap\"][\"metatags\"]\n",
    "                for tag in metatags:\n",
    "                    date = tag.get(\"article:published_time\") or tag.get(\"pubdate\")\n",
    "                    if date:\n",
    "                        break\n",
    "\n",
    "            # Handle relative dates in the description\n",
    "            if not date:  # If no date was found in the metatags\n",
    "                date = parse_relative_date(item.get(\"snippet\", \"\"))  # Call with 1 argument\n",
    "\n",
    "            results.append({\n",
    "                \"Title\": item.get(\"title\"),\n",
    "                \"Link\": item.get(\"link\"),\n",
    "                \"Description\": item.get(\"snippet\"),\n",
    "                \"Date\": date or \"N/A\",  # Default to \"N/A\" if no date is found\n",
    "            })\n",
    "\n",
    "        # Update start index for the next batch (increment by 10)\n",
    "        start_index += max_results\n",
    "\n",
    "        # Google Custom Search JSON API allows a maximum of 100 results\n",
    "        if start_index > 100:\n",
    "            print(\"Reached maximum retrievable results (100).\")\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def parse_relative_date(text):\n",
    "    \"\"\"Parses relative date strings (e.g., '1 day ago') into an actual date.\"\"\"\n",
    "    try:\n",
    "        match = re.search(r'(\\d+)\\s*(day|hour|minute|week|month|year)s?\\s*ago', text, re.IGNORECASE)\n",
    "        if match:\n",
    "            value, unit = int(match.group(1)), match.group(2).lower()\n",
    "            now = datetime.now()\n",
    "\n",
    "            if unit == 'day':\n",
    "                parsed_date = now - timedelta(days=value)\n",
    "            elif unit == 'hour':\n",
    "                parsed_date = now - timedelta(hours=value)\n",
    "            elif unit == 'minute':\n",
    "                parsed_date = now - timedelta(minutes=value)\n",
    "            elif unit == 'week':\n",
    "                parsed_date = now - timedelta(weeks=value)\n",
    "            elif unit == 'month':\n",
    "                parsed_date = now - timedelta(days=value * 30)  # Approximation\n",
    "            elif unit == 'year':\n",
    "                parsed_date = now - timedelta(days=value * 365)  # Approximation\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "            return parsed_date.strftime('%Y-%m-%d')  # Return only the date\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing relative date: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_results_to_json(results, file_name, filename):\n",
    "    \"\"\"Saves results to a JSON file in the specified folder.\"\"\"\n",
    "    # Ensure the folder exists\n",
    "    os.makedirs(file_name, exist_ok=True)\n",
    "\n",
    "    # Create full file path\n",
    "    file_path = os.path.join(file_name, filename)\n",
    "\n",
    "    # Save results to JSON\n",
    "    with open(file_path, mode='w', encoding='utf-8') as file:\n",
    "        json.dump(results, file, indent=4, ensure_ascii=False)\n",
    "    print(f\"Results saved to {file_path}\")\n",
    "\n",
    "\n",
    "# Fetch results\n",
    "DATE_RESTRICT = 'd2'\n",
    "search_results = fetch_google_results(API_KEY, CSE_ID, SEARCH_QUERY, NUM_RESULTS, DATE_RESTRICT)\n",
    "\n",
    "# Replace spaces with dashes for the folder and file name\n",
    "file_name = SEARCH_QUERY.replace(' ', '-')\n",
    "output_filename = f\"{PROJECT_NAME}-gsr-{file_name}.json\"\n",
    "\n",
    "# Save to JSON in the folder\n",
    "if search_results:\n",
    "    save_results_to_json(search_results, SEARCH_FILE_FOLDER_PATH, output_filename)\n",
    "else:\n",
    "    print(\"No results to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7c3ca-d340-4280-abfa-0b984b599b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching content for: https://www.hyundai-forums.com/threads/car-and-driver-santa-fe-hybrid-review.720456/\n",
      "Fetching content for: https://www.santacruzforums.com/threads/you-can-carry-4x8-materials.15898/latest\n",
      "Fetching content for: https://www.hyundaiperformance.com/threads/06-sonata-getting-access-to-ac-mounting-fastener.127070/latest\n",
      "Fetching content for: https://www.tucson-forum.com/threads/factory-washer-fluid.3958/latest\n",
      "Fetching content for: https://www.palisadeforum.com/threads/camping-out-in-my-2023-hyundai-palisade-xrt.8373/\n",
      "Fetching content for: https://www.tucson-forum.com/threads/windshield-phenomenon.3957/latest\n",
      "Fetching content for: https://www.palisadeforum.com/threads/help-i-am-stranded-and-60-miles-from-dealer.8375/\n",
      "Fetching content for: https://www.newtiburon.com/threads/high-rise-spoiler-needs-paint-tlc.484361/\n",
      "Fetching content for: https://www.ioniqforum.com/threads/2025-ioniq-5-can-supercharge-more-quickly-than-earlier-models.51737/latest\n",
      "Fetching content for: https://www.reddit.com/r/CarsIndia/\n",
      "Fetching content for: https://forum.leasehackr.com/tag/hyundai\n",
      "Fetching content for: https://www.speakev.com/threads/hyundai-ioniq-5-charges-faster-on-a-tesla-supercharger-than-a-model-3-does.189337/\n",
      "Fetching content for: https://www.macanevowners.com/forum/threads/is-chrono-plus-really-necessary-on-the-turbo.19035/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# ScraperAPI endpoint and API key\n",
    "SCRAPER_API_URL = \"http://api.scraperapi.com\"\n",
    "API_KEY = \"190d8602cbff5cbcfc555cbae04aeddc\"\n",
    "#Folder Path and Project Name Definition\n",
    "RAW_DATA_FOLDER_PATH=\"/home/madhavbpanicker/Documents/Scrape_project/Website-Data-Raw/\"\n",
    "PROJECT_NAME=\"reputation-management\"\n",
    "SEARCH_FILE_FOLDER_PATH=\"/home/madhavbpanicker/Documents/Scrape_project/Google-Search-Results/\"\n",
    "\n",
    "# Function to fetch page content using ScraperAPI\n",
    "def fetch_content(url):\n",
    "    try:\n",
    "        params = {\n",
    "            \"api_key\": API_KEY,\n",
    "            \"url\": url\n",
    "        }\n",
    "        response = requests.get(SCRAPER_API_URL, params=params, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            print(f\"Failed to fetch {url}: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Read the CSV file with URLs\n",
    "#input_file = f\"gsr-{SEARCH_QUERY.replace(' ','-')}.csv\"\n",
    "input_file = f\"{SEARCH_FILE_FOLDER_PATH}{output_filename}\"\n",
    "df = pd.read_json(input_file, orient='records')\n",
    "\n",
    "# Add columns for page content and fetch date\n",
    "df['page_content'] = \"\"\n",
    "df['fetch_date'] = \"\"\n",
    "\n",
    "# Loop through each URL and fetch content\n",
    "for index, row in df.iterrows():\n",
    "    url = row['Link']\n",
    "    print(f\"Fetching content for: {url}\")\n",
    "    \n",
    "    # Fetch content and add current date\n",
    "    content = fetch_content(url)\n",
    "    fetch_date = datetime.now().strftime('%Y-%m-%d')  # Get current date in YYYY-MM-DD format\n",
    "    \n",
    "    # Update DataFrame\n",
    "    df.at[index, 'page_content'] = content\n",
    "    df.at[index, 'fetch_date'] = fetch_date\n",
    "    \n",
    "    time.sleep(2)  \n",
    "\n",
    "# Save the updated DataFrame with page content to a new CSV file\n",
    "output_file = f\"{RAW_DATA_FOLDER_PATH}{PROJECT_NAME}-scraped_results-{SEARCH_QUERY.replace(' ','-')}.json\"\n",
    "# Convert DataFrame to a list of dictionaries (records) for JSON serialization\n",
    "df_records = df.to_dict(orient='records')\n",
    "\n",
    "# Save the JSON data to the file\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(df_records, f, ensure_ascii=False, indent=4)\n",
    "print(f\"Scraping completed. Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953fa6e2-d75e-431b-919e-fda38674911b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
