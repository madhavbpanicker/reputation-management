{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9e03bef-6178-4eed-97bc-be13889f5b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching results for: Honda reviews\n",
      "Reached maximum retrievable results (100).\n",
      "Results saved to /home/madhavbpanicker/Documents/Scrape_project/Google-Search-Results/reputation-management-gsr-Honda-reviews.json\n",
      "Fetching content for: https://honda-tech.com/\n",
      "Fetching content for: https://www.youtube.com/channel/UCX13wwdzipRhH001W3X0WyQ\n",
      "Fetching content for: https://www.suburbanhonda.com/customer-reviews.htm\n",
      "Fetching content for: https://www.dealerrater.com/dealer/Valley-Honda-review-14326/\n",
      "Fetching content for: https://www.dchacademyhonda.com/our-reviews\n",
      "Fetching content for: https://www.dealerrater.com/dealer/Tempe-Honda-review-15494/\n",
      "Fetching content for: https://www.valleyhonda.com/about-us/customer-testimonials/\n",
      "Fetching content for: https://www.dealerrater.com/dealer/Millennium-Honda-review-15415/\n",
      "Fetching content for: https://www.hondacityli.com/about-us/customer-testimonials/\n",
      "Error fetching https://www.hondacityli.com/about-us/customer-testimonials/: HTTPConnectionPool(host='api.scraperapi.com', port=80): Read timed out. (read timeout=30)\n",
      "Fetching content for: https://www.dealerrater.com/dealer/Gardena-Honda-review-5631/\n",
      "Fetching content for: https://www.germainhondaofbeavercreek.com/about-us/customer-reviews-and-testimonials/\n",
      "Error fetching https://www.germainhondaofbeavercreek.com/about-us/customer-reviews-and-testimonials/: HTTPConnectionPool(host='api.scraperapi.com', port=80): Read timed out. (read timeout=30)\n",
      "Fetching content for: https://www.dealerrater.com/dealer/Lou-Sobh-Honda-review-103789/\n",
      "Fetching content for: https://www.dchkayhonda.com/about-us/customer-testimonials/\n",
      "Fetching content for: https://www.dealerrater.com/dealer/Liberty-Honda-review-8468/\n",
      "Fetching content for: https://www.planethonda.com/planet-honda-about-us/customer-testimonials/\n",
      "Error fetching https://www.planethonda.com/planet-honda-about-us/customer-testimonials/: HTTPConnectionPool(host='api.scraperapi.com', port=80): Read timed out. (read timeout=30)\n",
      "Fetching content for: https://www.spreenhonda.com/about-us/customer-testimonials/\n",
      "Fetching content for: https://www.bianchihonda.com/about-us/customer-testimonials/\n",
      "Fetching content for: https://www.cars.com/dealers/9835/honda-of-concord/\n",
      "Fetching content for: https://atvconnection.com/\n",
      "Fetching content for: https://peheartlandhonda.powerdealer.honda.com/products/generators/eu7000is\n",
      "Fetching content for: https://peheartlandhonda.powerdealer.honda.com/products/generators/eu7000is\n",
      "Fetching content for: https://www.threads.net/@regularcarreviews\n",
      "Fetching content for: https://www.yelp.com/biz/nucar-norwood-2\n",
      "Fetching content for: https://www.libertyhonda.com/about-us/customer-testimonials/\n",
      "Fetching content for: https://www.sfhonda.com/about-us/customer-testimonials/\n",
      "Error fetching https://www.sfhonda.com/about-us/customer-testimonials/: HTTPConnectionPool(host='api.scraperapi.com', port=80): Read timed out. (read timeout=30)\n",
      "Fetching content for: https://www.hondaofolathe.com/about-us/customer-testimonials/\n",
      "Error fetching https://www.hondaofolathe.com/about-us/customer-testimonials/: HTTPConnectionPool(host='api.scraperapi.com', port=80): Read timed out. (read timeout=30)\n",
      "Fetching content for: https://www.dchhondaofnanuet.com/about-us/customer-testimonials/\n",
      "Error fetching https://www.dchhondaofnanuet.com/about-us/customer-testimonials/: HTTPConnectionPool(host='api.scraperapi.com', port=80): Read timed out. (read timeout=30)\n",
      "Fetching content for: https://www.autorepair-review.com/shop/125490/honda-of-oakland-auto-repair-service\n",
      "Fetching content for: https://www.executivehonda.com/inventory/certified-used-2021-honda-hr-v-lx-awd-4d-sport-utility-3czru6h32mm747853/\n",
      "Error fetching https://www.executivehonda.com/inventory/certified-used-2021-honda-hr-v-lx-awd-4d-sport-utility-3czru6h32mm747853/: HTTPConnectionPool(host='api.scraperapi.com', port=80): Read timed out. (read timeout=30)\n",
      "Fetching content for: https://www.buycolonialhonda.com/colonial-honda-dealership-customer-reviews\n",
      "Fetching content for: https://www.normreeveshondanorthrichlandhills.com/customer-testimonials/\n",
      "Fetching content for: https://www.carfax.com/Nearby-Honda-Dealerships_m11\n",
      "Fetching content for: https://www.facebook.com/EastCoastHonda/\n",
      "Failed to fetch https://www.facebook.com/EastCoastHonda/: 403\n",
      "Fetching content for: https://www.driveaccord.net/threads/anyone-looking-for-a-90s-factory-radio.576460/latest\n",
      "Fetching content for: https://www.oneautogroup.com/about-us/customer-testimonials/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 173\u001b[0m\n\u001b[1;32m    171\u001b[0m url \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLink\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching content for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 173\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m fetch_date \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    175\u001b[0m df\u001b[38;5;241m.\u001b[39mat[index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage_content\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m content\n",
      "Cell \u001b[0;32mIn[1], line 121\u001b[0m, in \u001b[0;36mfetch_content\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: SCRAPER_API_KEY,\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: url\n\u001b[1;32m    120\u001b[0m     }\n\u001b[0;32m--> 121\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSCRAPER_API_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rapids-24.10/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "import boto3\n",
    "from io import StringIO\n",
    "\n",
    "# Constants\n",
    "GOOGLE_API_KEY = \"\"  # Replace with your Google API key\n",
    "CSE_ID = \"\"  # Replace with your Custom Search Engine ID\n",
    "NUM_RESULTS = 100  # Total number of results needed (max 100)\n",
    "SEARCH_FILE_FOLDER_PATH = \"/home/madhavbpanicker/Documents/Scrape_project/Google-Search-Results/\"\n",
    "RAW_DATA_FOLDER_PATH = \"/home/madhavbpanicker/Documents/Scrape_project/Website-Data-Raw/\"\n",
    "PROJECT_NAME = \"reputation-management\"\n",
    "SCRAPER_API_URL = \"http://api.scraperapi.com\"\n",
    "SCRAPER_API_KEY = \"\"\n",
    "S3_BUCKET_NAME = \"\"\n",
    "DATE_RESTRICT = \"d2\" #Add restriction to make it only show results from the past two days\n",
    "CAR_COMPANIES = [\n",
    "    \"Honda\", \"Nissan\",\"Subaru\", \"Mazda\", \"Chevrolet\", \"Buick\", \"Volkswagen\"\n",
    "]\n",
    "\n",
    "# Function to fetch results from Google Custom Search API\n",
    "def fetch_google_results(api_key, cse_id, query, num_results, date_restrict=None):\n",
    "    results = []\n",
    "    start_index = 1\n",
    "    while len(results) < num_results:\n",
    "        max_results = min(num_results - len(results), 10)\n",
    "\n",
    "        url = (\n",
    "            f\"https://www.googleapis.com/customsearch/v1\"\n",
    "            f\"?key={api_key}&cx={cse_id}&q={query}&start={start_index}\"\n",
    "        )\n",
    "\n",
    "        if date_restrict:\n",
    "            url += f\"&dateRestrict={date_restrict}\"\n",
    "\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        if \"items\" not in data:\n",
    "            print(\"No more items in response.\")\n",
    "            break\n",
    "\n",
    "        for item in data[\"items\"]:\n",
    "            date = None\n",
    "            if \"pagemap\" in item and \"metatags\" in item[\"pagemap\"]:\n",
    "                metatags = item[\"pagemap\"][\"metatags\"]\n",
    "                for tag in metatags:\n",
    "                    date = tag.get(\"article:published_time\") or tag.get(\"pubdate\")\n",
    "                    if date:\n",
    "                        break\n",
    "\n",
    "            if not date:\n",
    "                date = parse_relative_date(item.get(\"snippet\", \"\"))\n",
    "\n",
    "            results.append({\n",
    "                \"Title\": item.get(\"title\"),\n",
    "                \"Link\": item.get(\"link\"),\n",
    "                \"Description\": item.get(\"snippet\"),\n",
    "                \"Date\": date or \"N/A\",\n",
    "            })\n",
    "\n",
    "        start_index += max_results\n",
    "\n",
    "        if start_index > 100:\n",
    "            print(\"Reached maximum retrievable results (100).\")\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "# Function to parse relative dates\n",
    "def parse_relative_date(text):\n",
    "    try:\n",
    "        match = re.search(r'(\\d+)\\s*(day|hour|minute|week|month|year)s?\\s*ago', text, re.IGNORECASE)\n",
    "        if match:\n",
    "            value, unit = int(match.group(1)), match.group(2).lower()\n",
    "            now = datetime.now()\n",
    "\n",
    "            if unit == 'day':\n",
    "                parsed_date = now - timedelta(days=value)\n",
    "            elif unit == 'hour':\n",
    "                parsed_date = now - timedelta(hours=value)\n",
    "            elif unit == 'minute':\n",
    "                parsed_date = now - timedelta(minutes=value)\n",
    "            elif unit == 'week':\n",
    "                parsed_date = now - timedelta(weeks=value)\n",
    "            elif unit == 'month':\n",
    "                parsed_date = now - timedelta(days=value * 30)\n",
    "            elif unit == 'year':\n",
    "                parsed_date = now - timedelta(days=value * 365)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "            return parsed_date.strftime('%Y-%m-%d')\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing relative date: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to save results to JSON\n",
    "def save_results_to_json(results, folder_path, filename):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    with open(file_path, mode='w', encoding='utf-8') as file:\n",
    "        json.dump(results, file, indent=4, ensure_ascii=False)\n",
    "    print(f\"Results saved to {file_path}\")\n",
    "\n",
    "# Function to fetch page content\n",
    "def fetch_content(url):\n",
    "    try:\n",
    "        params = {\n",
    "            \"api_key\": SCRAPER_API_KEY,\n",
    "            \"url\": url\n",
    "        }\n",
    "        response = requests.get(SCRAPER_API_URL, params=params, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            print(f\"Failed to fetch {url}: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to upload file to S3\n",
    "def upload_to_s3(file_path, bucket_name, s3_folder):\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id='',\n",
    "        aws_secret_access_key='',\n",
    "        region_name='ap-south-1'\n",
    "    )\n",
    "    try:\n",
    "        s3_file_key = f\"{s3_folder}/{os.path.basename(file_path)}\"\n",
    "        s3.upload_file(file_path, bucket_name, s3_file_key)\n",
    "        print(f\"File uploaded to S3: s3://{bucket_name}/{s3_file_key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading to S3: {e}\")\n",
    "\n",
    "# Main workflow\n",
    "for company in CAR_COMPANIES:\n",
    "    search_query = f\"{company} reviews\"\n",
    "    print(f\"Fetching results for: {search_query}\")\n",
    "\n",
    "    # Fetch Google Search results\n",
    "    search_results = fetch_google_results(GOOGLE_API_KEY, CSE_ID, search_query, NUM_RESULTS, DATE_RESTRICT)\n",
    "\n",
    "    if not search_results:\n",
    "        print(f\"No results for {company}.\")\n",
    "        continue\n",
    "\n",
    "    # Save search results to JSON\n",
    "    file_name = f\"{PROJECT_NAME}-gsr-{search_query.replace(' ', '-')}.json\"\n",
    "    save_results_to_json(search_results, SEARCH_FILE_FOLDER_PATH, file_name)\n",
    "\n",
    "    # Load search results\n",
    "    input_file = os.path.join(SEARCH_FILE_FOLDER_PATH, file_name)\n",
    "    df = pd.read_json(input_file, orient='records')\n",
    "\n",
    "    # Add columns for page content and fetch date\n",
    "    df['page_content'] = \"\"\n",
    "    df['fetch_date'] = \"\"\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        url = row['Link']\n",
    "        print(f\"Fetching content for: {url}\")\n",
    "        content = fetch_content(url)\n",
    "        fetch_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        df.at[index, 'page_content'] = content\n",
    "        df.at[index, 'fetch_date'] = fetch_date\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Save updated results to JSON\n",
    "    output_file = os.path.join(RAW_DATA_FOLDER_PATH, f\"{PROJECT_NAME}-scraped_results-{search_query.replace(' ', '-')}.json\")\n",
    "    df['fetch_date'] = df['fetch_date'].astype(str)\n",
    "    df_records = df.to_dict(orient='records')\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(df_records, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Scraping completed for {company}. Results saved to {output_file}\")\n",
    "\n",
    "    # Upload to S3\n",
    "    upload_to_s3(output_file, S3_BUCKET_NAME, \"Website-Data-Raw\")\n",
    "\n",
    "print(\"All companies processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3163564-9cc1-4c77-a238-e52642a5dd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: reputation-management-scraped_results-Kia-reviews.json\n",
      "Processed data saved to: /home/madhavbpanicker/Documents/Scrape_project/Website-Data-Trimmed/reputation-management-scraped_results-Kia-reviews_processed.json\n",
      "Processing file: reputation-management-scraped_results-Toyota-reviews.json\n",
      "Processed data saved to: /home/madhavbpanicker/Documents/Scrape_project/Website-Data-Trimmed/reputation-management-scraped_results-Toyota-reviews_processed.json\n",
      "Processing file: reputation-management-scraped_results-Hyundai-reviews.json\n",
      "Processed data saved to: /home/madhavbpanicker/Documents/Scrape_project/Website-Data-Trimmed/reputation-management-scraped_results-Hyundai-reviews_processed.json\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Constants\n",
    "RAW_DATA_FOLDER_PATH = \"/home/madhavbpanicker/Documents/Scrape_project/Website-Data-Raw/\"\n",
    "PROCESSED_OUTPUT_PATH = \"/home/madhavbpanicker/Documents/Scrape_project/Website-Data-Trimmed/\"\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(PROCESSED_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Function to clean HTML content\n",
    "def clean_html(content):\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    # Remove script and style elements\n",
    "    for script_or_style in soup([\"script\", \"style\"]):\n",
    "        script_or_style.decompose()\n",
    "    # Get clean text\n",
    "    return soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "# Function to process JSON files\n",
    "def process_json_files(folder_path, output_path):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # Load the JSON file\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            processed_entries = []\n",
    "\n",
    "            for entry in data:\n",
    "                # Skip entries with null or empty page_content\n",
    "                if not entry.get(\"page_content\"):\n",
    "                    continue\n",
    "\n",
    "                # Clean HTML content\n",
    "                cleaned_content = clean_html(entry[\"page_content\"])\n",
    "\n",
    "                # Extract sentences from the cleaned page_content\n",
    "                sentences = sent_tokenize(cleaned_content)\n",
    "\n",
    "                # Create a new entry for each sentence\n",
    "                for sentence in sentences:\n",
    "                    processed_entries.append({\n",
    "                        \"Title\": entry.get(\"Title\", \"N/A\"),\n",
    "                        \"Link\": entry.get(\"Link\", \"N/A\"),\n",
    "                        \"Description\": entry.get(\"Description\", \"N/A\"),\n",
    "                        \"Date\": entry.get(\"Date\", \"N/A\"),\n",
    "                        \"fetch_date\": entry.get(\"fetch_date\", \"N/A\"),\n",
    "                        \"Sentence\": sentence\n",
    "                    })\n",
    "\n",
    "            # Save the processed entries to a new JSON file\n",
    "            output_file = os.path.join(output_path, file_name.replace(\".json\", \"_processed.json\"))\n",
    "            with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "                json.dump(processed_entries, out_f, ensure_ascii=False, indent=4)\n",
    "            print(f\"Processed data saved to: {output_file}\")\n",
    "\n",
    "# Run the processing function\n",
    "process_json_files(RAW_DATA_FOLDER_PATH, PROCESSED_OUTPUT_PATH)\n",
    "\n",
    "print(\"All files processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5118907-f95d-4d61-a88f-e85ef71b6717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: reputation-management-scraped_results-Kia-reviews.json\n",
      "Processed data saved to: /home/madhavbpanicker/Documents/Scrape_project/Processed-Data/reputation-management-scraped_results-Kia-reviews_processed.json\n",
      "Processing file: reputation-management-scraped_results-Toyota-reviews.json\n",
      "Processed data saved to: /home/madhavbpanicker/Documents/Scrape_project/Processed-Data/reputation-management-scraped_results-Toyota-reviews_processed.json\n",
      "Processing file: reputation-management-scraped_results-Hyundai-reviews.json\n",
      "Processed data saved to: /home/madhavbpanicker/Documents/Scrape_project/Processed-Data/reputation-management-scraped_results-Hyundai-reviews_processed.json\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load SpaCy model for sentence detection\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"SpaCy model 'en_core_web_sm' not found. Downloading now...\")\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Constants\n",
    "RAW_DATA_FOLDER_PATH = \"/home/madhavbpanicker/Documents/Scrape_project/Website-Data-Raw/\"\n",
    "PROCESSED_OUTPUT_PATH = \"/home/madhavbpanicker/Documents/Scrape_project/Processed-Data/\"\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(PROCESSED_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Function to clean HTML content\n",
    "def clean_html(content):\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    # Remove script and style elements\n",
    "    for script_or_style in soup([\"script\", \"style\"]):\n",
    "        script_or_style.decompose()\n",
    "    # Get clean text\n",
    "    return soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "# Function to split text into sentences using SpaCy\n",
    "def extract_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "# Function to process JSON files\n",
    "def process_json_files(folder_path, output_path):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # Load the JSON file\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            processed_entries = []\n",
    "\n",
    "            for entry in data:\n",
    "                # Skip entries with null or empty page_content\n",
    "                if not entry.get(\"page_content\"):\n",
    "                    continue\n",
    "\n",
    "                # Clean HTML content\n",
    "                cleaned_content = clean_html(entry[\"page_content\"])\n",
    "\n",
    "                # Extract sentences from the cleaned page_content\n",
    "                sentences = extract_sentences(cleaned_content)\n",
    "\n",
    "                # Create a new entry for each sentence\n",
    "                for sentence in sentences:\n",
    "                    processed_entries.append({\n",
    "                        \"Title\": entry.get(\"Title\", \"N/A\"),\n",
    "                        \"Link\": entry.get(\"Link\", \"N/A\"),\n",
    "                        \"Description\": entry.get(\"Description\", \"N/A\"),\n",
    "                        \"Date\": entry.get(\"Date\", \"N/A\"),\n",
    "                        \"fetch_date\": entry.get(\"fetch_date\", \"N/A\"),\n",
    "                        \"Sentence\": sentence\n",
    "                    })\n",
    "\n",
    "            # Save the processed entries to a new JSON file\n",
    "            output_file = os.path.join(output_path, file_name.replace(\".json\", \"_processed.json\"))\n",
    "            with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "                json.dump(processed_entries, out_f, ensure_ascii=False, indent=4)\n",
    "            print(f\"Processed data saved to: {output_file}\")\n",
    "\n",
    "# Run the processing function\n",
    "process_json_files(RAW_DATA_FOLDER_PATH, PROCESSED_OUTPUT_PATH)\n",
    "\n",
    "print(\"All files processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "501f3a75-ec79-4786-a5d1-c4bce5f7ff89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: reputation-management-scraped_results-Kia-reviews.json\n",
      "Processed data saved to: /home/madhavbpanicker/Documents/Scrape_project/Processed-Data-Para/reputation-management-scraped_results-Kia-reviews_processed.json\n",
      "Processing file: reputation-management-scraped_results-Toyota-reviews.json\n",
      "Processed data saved to: /home/madhavbpanicker/Documents/Scrape_project/Processed-Data-Para/reputation-management-scraped_results-Toyota-reviews_processed.json\n",
      "Processing file: reputation-management-scraped_results-Hyundai-reviews.json\n",
      "Processed data saved to: /home/madhavbpanicker/Documents/Scrape_project/Processed-Data-Para/reputation-management-scraped_results-Hyundai-reviews_processed.json\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load SpaCy model for paragraph detection\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"SpaCy model 'en_core_web_sm' not found. Downloading now...\")\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Constants\n",
    "RAW_DATA_FOLDER_PATH = \"/home/madhavbpanicker/Documents/Scrape_project/Website-Data-Raw/\"\n",
    "PROCESSED_OUTPUT_PATH = \"/home/madhavbpanicker/Documents/Scrape_project/Processed-Data-Para/\"\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(PROCESSED_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Function to clean HTML content\n",
    "def clean_html(content):\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    # Remove script and style elements\n",
    "    for script_or_style in soup([\"script\", \"style\"]):\n",
    "        script_or_style.decompose()\n",
    "    # Get clean text\n",
    "    return soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "# Function to split text into paragraphs\n",
    "def extract_paragraphs(text):\n",
    "    paragraphs = [para.strip() for para in text.split(\"\\n\") if para.strip()]\n",
    "    return paragraphs\n",
    "\n",
    "# Function to process JSON files\n",
    "def process_json_files(folder_path, output_path):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # Load the JSON file\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            processed_entries = []\n",
    "\n",
    "            for entry in data:\n",
    "                # Skip entries with null or empty page_content\n",
    "                if not entry.get(\"page_content\"):\n",
    "                    continue\n",
    "\n",
    "                # Clean HTML content\n",
    "                cleaned_content = clean_html(entry[\"page_content\"])\n",
    "\n",
    "                # Extract paragraphs from the cleaned page_content\n",
    "                paragraphs = extract_paragraphs(cleaned_content)\n",
    "\n",
    "                # Create a new entry for each paragraph\n",
    "                for paragraph in paragraphs:\n",
    "                    processed_entries.append({\n",
    "                        \"Title\": entry.get(\"Title\", \"N/A\"),\n",
    "                        \"Link\": entry.get(\"Link\", \"N/A\"),\n",
    "                        \"Description\": entry.get(\"Description\", \"N/A\"),\n",
    "                        \"Date\": entry.get(\"Date\", \"N/A\"),\n",
    "                        \"fetch_date\": entry.get(\"fetch_date\", \"N/A\"),\n",
    "                        \"Paragraph\": paragraph\n",
    "                    })\n",
    "\n",
    "            # Save the processed entries to a new JSON file\n",
    "            output_file = os.path.join(output_path, file_name.replace(\".json\", \"_processed.json\"))\n",
    "            with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "                json.dump(processed_entries, out_f, ensure_ascii=False, indent=4)\n",
    "            print(f\"Processed data saved to: {output_file}\")\n",
    "\n",
    "# Run the processing function\n",
    "process_json_files(RAW_DATA_FOLDER_PATH, PROCESSED_OUTPUT_PATH)\n",
    "\n",
    "print(\"All files processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff535d53-6a39-4753-9399-5b77243bf9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: reputation-management-scraped_results-Kia-reviews.json\n",
      "Processed data saved to: /home/madhavbpanicker/Documents/Scrape_project/Processed-Data-Para-Relevance/reputation-management-scraped_results-Kia-reviews_processed.json\n",
      "Processing file: reputation-management-scraped_results-Toyota-reviews.json\n",
      "Processed data saved to: /home/madhavbpanicker/Documents/Scrape_project/Processed-Data-Para-Relevance/reputation-management-scraped_results-Toyota-reviews_processed.json\n",
      "Processing file: reputation-management-scraped_results-Hyundai-reviews.json\n",
      "Processed data saved to: /home/madhavbpanicker/Documents/Scrape_project/Processed-Data-Para-Relevance/reputation-management-scraped_results-Hyundai-reviews_processed.json\n",
      "All files processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load SpaCy model for paragraph detection\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"SpaCy model 'en_core_web_sm' not found. Downloading now...\")\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Constants\n",
    "RAW_DATA_FOLDER_PATH = \"/home/madhavbpanicker/Documents/Scrape_project/Website-Data-Raw/\"\n",
    "PROCESSED_OUTPUT_PATH = \"/home/madhavbpanicker/Documents/Scrape_project/Processed-Data-Para-Relevance/\"\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(PROCESSED_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Function to clean HTML content\n",
    "def clean_html(content):\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    # Remove script and style elements\n",
    "    for script_or_style in soup([\"script\", \"style\"]):\n",
    "        script_or_style.decompose()\n",
    "    # Get clean text\n",
    "    return soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "# Function to split text into paragraphs\n",
    "def extract_paragraphs(text):\n",
    "    paragraphs = [para.strip() for para in text.split(\"\\n\") if para.strip()]\n",
    "    return paragraphs\n",
    "\n",
    "# Function to filter relevant paragraphs\n",
    "def is_relevant_paragraph(paragraph):\n",
    "    # Define keywords indicating customer-related content\n",
    "    customer_keywords = [\n",
    "        \"I think\", \"I feel\", \"I believe\", \"experience\", \"review\", \"feedback\", \"service\", \n",
    "        \"support\", \"help\", \"recommend\", \"suggest\", \"advise\", \"share\"\n",
    "    ]\n",
    "    return any(keyword.lower() in paragraph.lower() for keyword in customer_keywords)\n",
    "\n",
    "# Function to process JSON files\n",
    "def process_json_files(folder_path, output_path):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".json\"):\n",
    "            print(f\"Processing file: {file_name}\")\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # Load the JSON file\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            processed_entries = []\n",
    "\n",
    "            for entry in data:\n",
    "                # Skip entries with null or empty page_content\n",
    "                if not entry.get(\"page_content\"):\n",
    "                    continue\n",
    "\n",
    "                # Clean HTML content\n",
    "                cleaned_content = clean_html(entry[\"page_content\"])\n",
    "\n",
    "                # Extract paragraphs from the cleaned page_content\n",
    "                paragraphs = extract_paragraphs(cleaned_content)\n",
    "\n",
    "                # Filter relevant paragraphs and create entries\n",
    "                for paragraph in paragraphs:\n",
    "                    if is_relevant_paragraph(paragraph):\n",
    "                        processed_entries.append({\n",
    "                            \"Title\": entry.get(\"Title\", \"N/A\"),\n",
    "                            \"Link\": entry.get(\"Link\", \"N/A\"),\n",
    "                            \"Description\": entry.get(\"Description\", \"N/A\"),\n",
    "                            \"Date\": entry.get(\"Date\", \"N/A\"),\n",
    "                            \"fetch_date\": entry.get(\"fetch_date\", \"N/A\"),\n",
    "                            \"Paragraph\": paragraph\n",
    "                        })\n",
    "\n",
    "            # Save the processed entries to a new JSON file\n",
    "            output_file = os.path.join(output_path, file_name.replace(\".json\", \"_processed.json\"))\n",
    "            with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "                json.dump(processed_entries, out_f, ensure_ascii=False, indent=4)\n",
    "            print(f\"Processed data saved to: {output_file}\")\n",
    "\n",
    "# Run the processing function\n",
    "process_json_files(RAW_DATA_FOLDER_PATH, PROCESSED_OUTPUT_PATH)\n",
    "\n",
    "print(\"All files processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59885075-be98-4215-a45c-a0c7baf4684f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
